{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Terrorism_Predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "interpreter": {
      "hash": "f589b74b5f9dfadb2b978c0f6d0c86213b22d6e1bb02d62665d77952f7aaa902"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "F4Y5r5afyKqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# # ! pip install pandas-profiling\n",
        "# ! pip install geopandas\n",
        "# # ! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
        "# ! pip install pandas\n",
        "# # ! pip install pandas==0.25\n",
        "# ! pip install tensorflow_text\n",
        "# # !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# # !unzip -q glove.6B.zip"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E7LlRxgASGQ",
        "outputId": "bf1263d8-69b2-4393-e9f8-5502982d9efd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# 'imports; megabytes of imports'  - Johnny 5\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from google.colab import files\n",
        "import pandas_profiling\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import folium\n",
        "from folium.plugins import FastMarkerCluster, Fullscreen, MiniMap, HeatMap, HeatMapWithTime\n",
        "import geopandas as gpd\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import operator\n",
        "import pathlib\n",
        "\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import tensorflow_datasets as tfds\n",
        "# import tensorflow_hub as hub\n",
        "# from tensorflow.keras import layers\n",
        "# from tensorflow.keras import losses\n",
        "# from tensorflow.keras import preprocessing\n",
        "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# import tensorflow_text as tf_text\n",
        "# from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.metrics import log_loss, make_scorer, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,  GridSearchCV\n",
        "from sklearn.feature_selection import f_classif, chi2\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/scottsturtz/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XKiRCUdV2Cr",
        "outputId": "66c0a7d3-1f65-41bf-fb51-1b867188f738"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "pd.set_option('display.max_colwidth', None) "
      ],
      "outputs": [],
      "metadata": {
        "id": "wy2m03fwqxkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# import dataset\n",
        "tdata = pd.read_csv('globalterrorismdb_0718dist.csv', low_memory=False)\n",
        "# tdata = pd.read_csv('/content/drive/MyDrive/globalterrorismdb_0718dist.csv', engine='python')\n",
        "# tdata.set_index('eventid')\n",
        "pd.options.display.max_columns = 1000\n",
        "pd.options.display.max_rows = 1000"
      ],
      "outputs": [],
      "metadata": {
        "id": "EoXpaESFV2Cu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# pandas_profiling.ProfileReport(tdata)"
      ],
      "outputs": [],
      "metadata": {
        "id": "z0oRYeWwiEAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data EDA and Cleaning"
      ],
      "metadata": {
        "id": "M5bIVC9JyZFp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# columns that can be removed: \n",
        "out_columns =  ['weapsubtype1', 'weaptype1', 'claimed',   'nperpcap', 'nwoundus', 'propvalue', 'propcomment', 'addnotes', 'INT_LOG', 'INT_IDEO', 'INT_MISC', 'INT_ANY', 'related',  'property',  'propextent','nperps', 'natlty1', 'targsubtype1','motive','dbsource','approxdate' , 'extended' , 'resolution' , 'vicinity' , 'location' , 'alternative', 'alternative_txt', 'multiple', 'attacktype2', 'attacktype2_txt' , 'attacktype3' ,'attacktype3_txt' ,'targtype2' ,\t'targtype2_txt' , \t'targsubtype2'\t, 'targsubtype2_txt' ,\t'corp2' , \t 'target2',\t'natlty2',\t'natlty2_txt','targtype3',\t'targtype3_txt',\t'targsubtype3',\t'targsubtype3_txt',\t'corp3',\t'target3',\t'natlty3',\t'natlty3_txt',\t'gsubname',\t'gname2',\t'gsubname2',\t'gname3',\t'gsubname3',\t'guncertain2',\t'guncertain3', 'claimmode',\t'claimmode_txt',\t'claim2',\t'claimmode2',\t'claimmode2_txt','claim3',\t'claimmode3',\t'claimmode3_txt',\t'compclaim', 'weaptype2',\t'weaptype2_txt',\t'weapsubtype2',\t'weapsubtype2_txt',\t'weaptype3', 'weapsubtype3',\t'weapsubtype3_txt', 'weaptype4','weaptype4_txt',\t'weapsubtype4',\t'weapsubtype4_txt', 'nkillus',\t'nkillter',\t'nwound',\t'nwoundte', 'ishostkid',\t'nhostkid',\t'nhostkidus',\t'nhours',\t'ndays',\t'divert', 'kidhijcountry',\t'ransom',\t'ransomamt',\t'ransomamtus',\t'ransompaid',\t'ransompaidus',\t'ransomnote',\t'hostkidoutcome',\t'hostkidoutcome_txt', 'nreleased', 'weaptype3_txt' ]\n",
        "\n",
        "tdata = tdata.drop(columns = out_columns) \n",
        "tdata.fillna('', inplace=True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "CV8-BKMvV2Cw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "\n",
        "counts = tdata['gname'].value_counts()\n",
        "tdata = tdata[tdata['gname'].isin(counts[counts > 400].index)]\n",
        "tdata.drop(tdata[tdata['summary'] == ''].index, inplace = True)\n",
        "tdata['summary']= tdata['summary'].str.replace('\\d+', '') #remove digits\n",
        "tdata['summary']= tdata['summary'].str.replace('[^\\w\\s]', '')#remove punctuation"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-071a8e781505>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  tdata['summary']= tdata['summary'].str.replace('\\d+', '') #remove digits\n",
            "<ipython-input-7-071a8e781505>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  tdata['summary']= tdata['summary'].str.replace('[^\\w\\s]', '')#remove punctuation\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kS6M90oDbBe",
        "outputId": "110ace10-d05c-4a99-bf3a-79223cf8abe0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "#copies to work with\n",
        "df= tdata.copy() # used during assignment\n",
        "df2=tdata.copy() # further experimenting\n",
        "df2=tdata.select_dtypes(exclude=['int64'])\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "df2= df2.replace('\\d+', '') #remove digits\n",
        "df2= df2.replace('[^\\w\\s]', '')#remove punctuation\n",
        "\n",
        "# df.isnull().sum()\n",
        "# df2.info()\n",
        "\n",
        "# df['data'] = df[['country_txt', 'region_txt ','provstate', 'city', '', '']].agg('-'.join, axis=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hwxw4i9oV2Cy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# data to predict on after training\n",
        "need_predict = df[df['gname'] == 'Unknown']\n",
        "# need_predict.info\n",
        "# need_predict.head()"
      ],
      "outputs": [],
      "metadata": {
        "id": "BV3VW1MkV2Cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68af9a0-5e7d-4421-c74d-281a53518cea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "#remove prediction data from main dataframe\n",
        "df.drop(df[df['gname'] == 'Unknown'].index, inplace = True)\n",
        "df2.drop(df2[df2['gname'] == 'Unknown'].index, inplace = True)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KAuHnJv-Guky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# reported groups\n",
        "# len(df.gname.value_counts())"
      ],
      "outputs": [],
      "metadata": {
        "id": "pFIWd9apGuoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc5c6c0-b191-4c46-e7be-77ff6109fd8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "#create 10% holdback tp verify multiple models. \n",
        "#Don't touch until end!\n",
        "hold_back = df.sample(frac = 0.1)\n",
        "hold_back2 = df2.sample(frac = 0.1)\n",
        "#hold_back.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "YjMmeeIPGuro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "df = df.drop(hold_back.index)\n",
        "df2 = df2.drop(hold_back2.index)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0dObsdbxGuun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f076da-84e3-4e8c-b188-b30773ac5a0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# df.shape,df2.shape\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "mU2cZSMeGuxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visuals"
      ],
      "metadata": {
        "id": "_gFg7jOQQLH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 30 active Terrorist Groups"
      ],
      "metadata": {
        "id": "ZY8B7QD5QX8l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# Groups = df['gname'].value_counts()\n",
        "# # rank = Groups[1].rank(ascending=False).values\n",
        "# # rank = (rank-1).astype(np.int)\n",
        "# GroupList = sorted(list(zip(Groups.values[:35],Groups.index[:35])),reverse=True)\n",
        "# MostActive, Counts = zip(*GroupList)\n",
        "# MostActive, Counts = list(MostActive), list(Counts)\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20,15))\n",
        "# t = sns.barplot(x=MostActive, y=Counts, palette='flare' )\n",
        "# # sns.barplot(x=Counts,y=MostActive)\n",
        "# ax.set_title('Most Active Terrorist Groups', fontsize=20)\n",
        "# plt.xlabel(\"Attacks\", fontsize=16)\n",
        "# plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "# plt.yticks(fontsize=16)\n",
        "# plt.xticks(np.arange(0, 7001, 200), rotation=60, fontsize=13)\n",
        "# fig.patch.set_facecolor('lightgray')\n",
        "# ax.set_facecolor('azure')\n",
        "# plt.tight_layout()\n",
        "# for v in str(Groups['count']):\n",
        "# plt.savefig('Most_Active.png')\n",
        "# plt.show()\n",
        "# Groups['count']\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "tcn8rP9nNbKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total Attacks vs Year\n",
        "\n"
      ],
      "metadata": {
        "id": "bJdQUKapQvLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# tdataYear = list(zip(tdata.iyear.value_counts().index,tdata.iyear.value_counts().values))\n",
        "# tdataYear = sorted(tdataYear)\n",
        "# terrorByYear = []\n",
        "# terrorByCounts = []\n",
        "# for i in tdataYear:\n",
        "#     terrorByYear.append(i[0])\n",
        "#     terrorByCounts.append(i[1])\n",
        "\n",
        "# dfYear = list(zip(df.iyear.value_counts().index,df.iyear.value_counts().values))\n",
        "# dfYear = sorted(dfYear)\n",
        "# terrorByYear2 = []\n",
        "# terrorByCounts2 = []\n",
        "# for i in dfYear:\n",
        "#     terrorByYear.append(i[0])\n",
        "#     terrorByCounts.append(i[1])\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(20,12))\n",
        "# sns.pointplot(x=terrorByYear,y=terrorByCounts,palette=sns.color_palette(\"icefire_r\"))\n",
        "# ax.set_title('Activity/Year Full Data', fontsize=20)\n",
        "# plt.xticks(fontsize=15, rotation=70)\n",
        "# plt.xlabel(\"Year\",fontsize=18 )\n",
        "# plt.ylabel(\"# of Attacks\",fontsize=18)\n",
        "# plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "# plt.yticks(np.arange(0.5,15001, 500),fontsize=15)\n",
        "# fig.patch.set_facecolor('lightgray')\n",
        "# ax.set_facecolor('azure')\n",
        "# plt.grid()\n",
        "\n",
        "\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.savefig('Attack Escalation Full.png')\n",
        "# # plt.show()\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "tsmWq7wLNbOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attacks By Year and Region"
      ],
      "metadata": {
        "id": "UXIWR5Q5ReWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "\n",
        "# regionYear = pd.crosstab(df.iyear,df.region_txt)\n",
        "# regionYear.plot(color=sns.color_palette(\"icefire_r\"),grid=True)\n",
        "\n",
        "# fig=plt.gcf()\n",
        "# fig.set_size_inches(20,8)\n",
        "# plt.xlabel(\"Year\",fontsize=18 )\n",
        "# plt.ylabel(\"Attacks\",fontsize=18)\n",
        "# plt.xticks(np.arange(1970, 2017.1, 2),fontsize=15, rotation=70)\n",
        "# plt.title(\"Attacks by Year and Region\", fontsize=20)\n",
        "# plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "# plt.yticks(np.arange(0.5,2501, 200),fontsize=15)\n",
        "# fig.patch.set_facecolor('lightgray')\n",
        "# ax.set_facecolor('azure')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# plt.grid()\n",
        "# plt.savefig('Region_Attacks.png') "
      ],
      "outputs": [],
      "metadata": {
        "id": "6qZNLXWhQOcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heat Map"
      ],
      "metadata": {
        "id": "1NX7irfuSEcw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "# country = tdata['country_txt']\n",
        "# country_weapon = tdata.groupby(by='weaptype1_txt', as_index=False).count().sort_values(by='eventid',ascending=False).iloc[:,:2]\n",
        "                                                  \n",
        "                                                                                                                \n",
        "# sns.barplot(x='weaptype1_txt', y='eventid', data=country_weapon, ax=ax, ci=None)\n",
        "# ax[1, 1].set_xticklabels(ax[1, 1].get_xticklabels(), rotation=90)\n",
        "# ax[1, 1].set_xlabel('')\n",
        "# ax[1, 1].set_ylabel('Count')\n",
        "# format_spines(ax[1, 1], right_border=False)\n",
        "# ax[1, 1].set_title(f'{country_name} Weapons Used in Attacks')\n",
        "\n",
        "# plt.suptitle(f'Terrorism Analysis in {country_name} between 1970 and 2017', size=16)\n",
        "# plt.tight_layout()\n",
        "# plt.subplots_adjust(top=0.90)\n",
        "# plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "b6gWDqsBQOkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "# pandas_profiling.ProfileReport(df)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZSyeT1IkQOnP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "counts2 = df['gname'].value_counts()\n",
        "# print(counts2)\n",
        "# counts.head(40)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "# df2.head(100)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "p8IZE_AzVk0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "#create train test split from df\n",
        "y = df['gname']\n",
        "X = df.drop(columns= ['eventid','longitude', 'latitude', 'gname'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=42)"
      ],
      "outputs": [],
      "metadata": {
        "id": "y1HlzOnXZzRj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "df2['name_id'] = df2['gname'].factorize()[0]\n",
        "\n",
        "g_data = df2[['gname', 'name_id']].drop_duplicates().sort_values('name_id')\n",
        "\n",
        "gname2id= dict(g_data.values)\n",
        "id2gname= dict(g_data[['name_id', 'gname']]).values\n",
        "y2= df2['gname']\n",
        "\n",
        "X2 = df2.drop(columns= ['longitude', 'latitude', 'gname', 'specificity', 'doubtterr', 'guncertain1', 'nkill','scite3'])\n",
        "X2['all_data'] = X2[['country_txt','region_txt','provstate','city','summary','attacktype1_txt','targtype1_txt','targsubtype1_txt','corp1','target1','natlty1_txt','weaptype1_txt','weapsubtype1_txt','weapdetail','propextent_txt','scite1','scite2']].agg('- '.join, axis=1)\n",
        "X2['all_data']= X2['all_data'].str.replace('\\d+', '') #remove digits\n",
        "X2['all_data']= X2['all_data'].str.replace('[^\\w\\s]', '')#remove punctuation\n",
        "X2['all_data'].str.strip()# remove extra whitespace\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2['all_data'],y2, test_size=.30, random_state=42) \n",
        "# list(X2.columns.values)\n",
        "# X2.head(2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-54b71cfcad98>:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  X2['all_data']= X2['all_data'].str.replace('\\d+', '') #remove digits\n",
            "<ipython-input-24-54b71cfcad98>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  X2['all_data']= X2['all_data'].str.replace('[^\\w\\s]', '')#remove punctuation\n"
          ]
        }
      ],
      "metadata": {
        "id": "BshVqZoURzXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest "
      ],
      "metadata": {
        "id": "d6VSX7zO3VL7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "rfd = X_train['summary']\n",
        "pred_rfd = X_test['summary']\n",
        "y_train = y_train\n",
        "y_test = y_test\n",
        "#vectorize summary data\n",
        "new_stopwords=['faraj',\n",
        "'farc' ,'farah', 'shewan','zuweid']\n",
        "stpwrd = stopwords.words('english')\n",
        "stpwrd.extend(new_stopwords)\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english', max_features = 100)\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words=stpwrd )\n",
        "\n",
        "\n",
        "vectorizer.fit(rfd)\n",
        "blind = need_predict.head()\n",
        "\n",
        "\n",
        "# RFX_train = vectorizer.transform(rfd)\n",
        "# RFX_test = vectorizer.transform(pred_rfd)\n",
        "# RFX_blind = vectorizer.transform(blind)\n",
        "# X2_train= vectorizer.transform(X2_train)\n",
        "# X2_test = vectorizer.transform(X2_test)\n",
        "\n",
        "features = tfidf.fit_transform(X2_train).toarray()\n",
        "labels = y2_train\n",
        "features"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.11621966, 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "# RFX_train.shape\n",
        "X2_train.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23041,)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "#need to troubleshoot. Why am I getting same words for all\n",
        "N=2\n",
        "for gname, name_id in sorted(gname2id.items()):\n",
        "  features_chi2 = chi2(features, labels == name_id)\n",
        "  indices = np.argsort(features_chi2[0])\n",
        "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "  print(\"# '{}':\".format(gname))\n",
        "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
        "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:]))) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 'Abu Sayyaf Group (ASG)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Al-Qaida in Iraq':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Al-Qaida in the Arabian Peninsula (AQAP)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Al-Shabaab':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Basque Fatherland and Freedom (ETA)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Boko Haram':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Communist Party of India - Maoist (CPI-Maoist)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Corsican National Liberation Front (FLNC)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Donetsk People's Republic':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Fulani extremists':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Hamas (Islamic Resistance Movement)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Hezbollah':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Houthi extremists (Ansar Allah)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Irish Republican Army (IRA)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Islamic State of Iraq and the Levant (ISIL)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Kurdistan Workers' Party (PKK)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Liberation Tigers of Tamil Eelam (LTTE)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Maoists':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Muslim extremists':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'National Liberation Army of Colombia (ELN)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'National Union for the Total Independence of Angola (UNITA)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'New People's Army (NPA)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Palestinian Extremists':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Palestinians':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Revolutionary Armed Forces of Colombia (FARC)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Separatists':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Shining Path (SL)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Sikh Extremists':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Sinai Province of the Islamic State':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Taliban':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n",
            "# 'Tehrik-i-Taliban Pakistan (TTP)':\n",
            "  . Most correlated unigrams:\n",
            ". fiftyeight\n",
            ". fight\n",
            "  . Most correlated bigrams:\n",
            ". fighters seize\n",
            ". zurmat district\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "# Train\n",
        "RFC = RandomForestClassifier(n_estimators=40, max_depth=20)\n",
        "RF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',max_depth=None, max_features='auto', max_leaf_nodes=None,min_impurity_decrease=0.0, min_impurity_split=None,min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,oob_score=False, random_state=None, verbose=0,warm_start=False)\n",
        "\n",
        "\n",
        "\n",
        "# RFC.fit(RFX_train, y_train)\n",
        "# RFC.fit(X2_train, y2_train)\n",
        "RF.fit(X2_train, y2_train)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'Yemen Middle East  North Africa Marib Marib  Assailants launched a missile targeting AlRuweik military camp in Marib Yemen There were no reported casualties in the attack Houthi extremists claimed responsibility for the incident BombingExplosion Military Military BarracksBaseHeadquartersCheckpost Military of Yemen Ar Ruwayk Camp Yemen Explosives Projectile rockets mortars RPGs etc A ballistic missile was used in the attack  Saudiled coalition targets Houthi launchpads in Yemen Anadolu Agency AA December   Yemeni forces missile hit Saudi base in Marib prov GlobalSecurityorg December  '",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-d9184bb6a467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# RFC.fit(RFX_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# RFC.fit(X2_train, y2_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             )\n\u001b[0;32m--> 304\u001b[0;31m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[1;32m    305\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    766\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    767\u001b[0m         \"\"\"\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Yemen Middle East  North Africa Marib Marib  Assailants launched a missile targeting AlRuweik military camp in Marib Yemen There were no reported casualties in the attack Houthi extremists claimed responsibility for the incident BombingExplosion Military Military BarracksBaseHeadquartersCheckpost Military of Yemen Ar Ruwayk Camp Yemen Explosives Projectile rockets mortars RPGs etc A ballistic missile was used in the attack  Saudiled coalition targets Houthi launchpads in Yemen Anadolu Agency AA December   Yemeni forces missile hit Saudi base in Marib prov GlobalSecurityorg December  '"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "y_pred = RF.predict(X2_test)\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y2_test, y_pred))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9540253164556962\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "source": [],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9808607594936709\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "pred = RFC.predict(RFX_blind)\n",
        "\n",
        "# print(pred)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "# #need_predict\n",
        "# blindtest = need_predict.head(5)\n",
        "# blindtest=vectorizer.transform(blindtest['summary'])\n",
        "# test2 = RFC.predict(blindtest)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "# feature importance\n",
        "feature_imp = X_train(RFC.feature_importances_,index=X_train(ascending=False))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'DataFrame' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b03b3e407863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# feature importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRFC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "# result = permutation_importance(RFC, RFX_train.toarray(), y_train, n_repeats=2,random_state=42, n_jobs=2)\n",
        "# sorted_idx = result.importances_mean.argsort()\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.boxplot(result.importances[sorted_idx].T,\n",
        "#            vert=False, labels=RFX_train.columns[sorted_idx])\n",
        "# ax.set_title(\"Permutation Importances (train set)\")\n",
        "# fig.tight_layout()\n",
        "# plt.show()\n",
        "# result = permutation_importance(RFC, RFX_test.toarray(), y_test)\n",
        "# def permutation_importance(X, y, RFC): \n",
        "#     perm = {}\n",
        "#     y_true = model.predict_proba(X)[:,1]\n",
        "#     baseline= roc_auc_score(y, y_true)\n",
        "#     for cols in X.columns:\n",
        "#         value = X[cols]\n",
        "#         X[cols] = np.random.permutation(X[cols].values)\n",
        "#         y_true = model.predict_proba(X)[:,1]\n",
        "#         perm[cols] = roc_auc_score(y, y_true) - baseline\n",
        "#         X[cols] = value\n",
        "#     return perm\n",
        "# prmutation_importance(RFX_test.toarray(), y_test, RFC)\n",
        "# parameters = {'max_depth' : np.arange(1, 12),\n",
        "#              'min_samples_leaf' : np.arange(5,95,10)}\n",
        "             \n",
        "# scorer = make_scorer(log_loss,\n",
        "#                      greater_is_better=False,\n",
        "#                      needs_proba=True)\n",
        "# clf = GridSearchCV(RFC,\n",
        "#                    parameters,\n",
        "#                    cv=10,\n",
        "#                    scoring=scorer).fit(RFX_train,y_train)\n",
        "# #Train and fit model                                                   \n",
        "# rf = RandomForestClassifier(n_estimators=1000,\n",
        "#                            max_features='auto',\n",
        "#                            random_state=0)\n",
        "# rf.fit(RFX_train, y_train)\n",
        "                                     \n",
        "# Test Prediction\n",
        "# pred = rf.predict(RFX_test)\n",
        "\n",
        "\n",
        "\n",
        "# cm = confusion_matrix(y_test, pred, labels=clf.classes_)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n",
        "# disp.plot();\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow"
      ],
      "metadata": {
        "id": "z6zlRdjSYqNM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "# data = df[['summary', 'gname']]\n",
        "# ds = tf.data.Dataset.from_tensor_slices(dict(data))\n",
        "\n",
        "# train, test = train_test_split(data, test_size=0.4)\n",
        "# test, val = train_test_split(test, test_size=0.5)\n",
        "# print(len(train), 'train examples')\n",
        "# print(len(val), 'validation examples')\n",
        "# print(len(test), 'test examples')\n",
        "# print(len(data))"
      ],
      "outputs": [],
      "metadata": {
        "id": "2wNNelZEw_cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0325d021-c78e-4be4-b265-4da8b083c036"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "data = df[['summary', 'gname']]\n",
        "# ds = tf.data.Dataset.from_tensor_slices(dict(data))\n",
        "\n",
        "# train, test = train_test_split(data, test_size=0.4)\n",
        "# test, val = train_test_split(test, test_size=0.5)\n",
        "# print(len(train), 'train examples')\n",
        "# print(len(val), 'validation examples')\n",
        "# print(len(test), 'test examples')\n",
        "# print(len(data))\n",
        "\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((train['summary'].values, train['gname'].values))\n",
        "# train_ds= tf.data.Dataset.from_tensor_slices(dict(train))\n",
        "# val_ds= tf.data.Dataset.from_tensor_slices(dict(val))\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices(dict(test))\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((train['summary'].values, train['gname'].values))\n",
        "# print(train)\n",
        "\n",
        "# max_features = 23502\n",
        "# max_len  = 150\n",
        "# embedding_dims = 100\n",
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features, lower=True)\n",
        "# GLOVE_EMBEDDING =  \"glove.6B.100d.txt\"\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "wnI9CbLuQOpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "#Data prep\n",
        "# x_train = train['summary'].str.lower()\n",
        "# y_train = train['gname'].values"
      ],
      "outputs": [],
      "metadata": {
        "id": "a2nvFHQIPFwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZqO50OHf67UX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "# tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# x_train =tokenizer.texts_to_sequences(x_train)\n",
        "# x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)"
      ],
      "outputs": [],
      "metadata": {
        "id": "wE242WxWcO8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "KNdeNprpuJku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "# #create embedding matrix\n",
        "# embeddings_index = {}\n",
        "\n",
        "# with open(GLOVE_EMBEDDING, encoding='utf8') as f:\n",
        "#     for line in f:\n",
        "#         values = line.rstrip().rsplit(' ')\n",
        "#         word = values[0]\n",
        "#         embed = np.asarray(values[1:], dtype='float32')\n",
        "#         embeddings_index[word] = embed\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "\n",
        "# num_words = min(max_features, len(word_index) + 1)\n",
        "\n",
        "# embedding_matrix = np.zeros((num_words, embedding_dims), dtype='float32')\n",
        "\n",
        "# for word, i in word_index.items():\n",
        "\n",
        "#     if i >= max_features:\n",
        "#         continue\n",
        "\n",
        "#     embedding_vector = embeddings_index.get(word)\n",
        "\n",
        "#     if embedding_vector is not None:\n",
        "#         embedding_matrix[i] = embedding_vector"
      ],
      "outputs": [],
      "metadata": {
        "id": "qCoPYlb6eENE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "qpdWu5AD4Sco"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "# input = tf.keras.layers.Input(shape=(max_len,))\n",
        "\n",
        "# x = tf.keras.layers.Embedding(max_features, embedding_dims, weights=[embedding_matrix], trainable=False)(input)"
      ],
      "outputs": [],
      "metadata": {
        "id": "TK8D4kfTwhYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "# #Biderictional Layer\n",
        "\n",
        "# x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
        "\n",
        "# x = tf.keras.layers.Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
        "\n",
        "# avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "# max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# x = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "\n",
        "# preds = tf.keras.layers.Dense(6, activation=\"sigmoid\")(x)\n",
        "\n",
        "# x.summary()\n",
        "\n",
        "# model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=1e-3), metrics=['accuracy'])\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "6rBXjMeVpsOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "#convert the pandas object to a tensor\n",
        "# train=tf.convert_to_tensor(train)\n",
        "# type(data)\n",
        "# train_data, test_data = tfds(train, split=[\"train\", \"test\"], \n",
        "#                                   batch_size=-1, as_supervised=True)\n",
        "# train_ds = tf.constant(([train['summary'], train['gname']]))\n",
        "# x_train= tf.data.Dataset.from_tensor_slices(dict(x_train))\n",
        "# train_examples, train_labels = tfds.as_numpy(x_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "fOZYgegQRCbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "# for feat, targ in train.take(5):\n",
        "#   print ('Features: {}, Target: {}'.format(feat, targ))"
      ],
      "outputs": [],
      "metadata": {
        "id": "vVYyZfM5MrHx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "# embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
        "# embeddings = embed(train_examples)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0drMmFjLzjvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "# print(embeddings.shape, embeddings.dtype)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9xAaML4W75mX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "# # build a model\n",
        "# model = tf.keras.Sequential()\n",
        "# model.add(hub_layer)\n",
        "# model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "# model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "# model.summary()"
      ],
      "outputs": [],
      "metadata": {
        "id": "S3lsQVPGzjst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "# # loss function and optimizer\n",
        "\n",
        "# model.compile(optimizer = 'adam',\n",
        "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "#               metrics=['accuracy'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "CGaMbTZLzjqS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "# mod_train = model.fit()(train_data.shuffle(10000).batch(512), \n",
        "#                         epochs =10\n",
        "#                         validation_data = validation_data.batch(512),\n",
        "#                         verbose=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "VAqpIU8ozjnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "source": [
        "# results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "# for name, value in zip(model.metric_names, results):\n",
        "#     print(\"%s: %.3f\" % (name, value))"
      ],
      "outputs": [],
      "metadata": {
        "id": "8v0lykbizjka"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "vY3rOxu8zjb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "O7_8PzonGu0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d6HpyiJFmIvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multinomial Naive Bayes Classifier "
      ],
      "metadata": {
        "id": "HzW3jTc4Se70"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "\n",
        "nbd_train = X_train['summary']\n",
        "nbd_test = X_test['summary']\n",
        "nby_train = y_train\n",
        "nby_test = y_test\n",
        "\n",
        "\n",
        "#vectorize summary data and transform to document term matrix\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "vectorizer.fit(nbd_train)\n",
        "\n",
        "nbd_train_dtm = vectorizer.transform(nbd_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "tEGO_IzgSgQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "source": [
        "#check out data\n",
        "# vectorizer.get_feature_names()"
      ],
      "outputs": [],
      "metadata": {
        "id": "CAEkNIocSg4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "# prep test data\n",
        "nbd_test_dtm = vectorizer.transform(nbd_test)\n",
        "nbd_test_dtm"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<9875x27438 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 249002 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "metadata": {
        "id": "uZ0mdiWyVqoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ef5de0-eb06-4150-e4a9-cec7927f0154"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "source": [
        "#finally get to build the model\n",
        "nb=MultinomialNB()\n",
        "\n",
        "\n",
        "%time nb.fit(nbd_train_dtm, nby_train )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 95.3 ms, sys: 10 ms, total: 105 ms\n",
            "Wall time: 106 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "metadata": {
        "id": "_xmlm8vjgHbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e95f4a-409d-4c4e-fd14-4a0be2bbbf27"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "source": [
        "# make prediction with test data\n",
        "prediction = nb.predict(nbd_test_dtm)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mLqM3cxMgHY8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "source": [
        "metrics.accuracy_score(nby_test, prediction)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9475443037974683"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "metadata": {
        "id": "K6YMsNeAgHVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66698b6c-b578-479a-adfb-c00463403da4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "WuqFCQAggHSR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "source": [
        "# examine class distribution\n",
        "print(y_test.value_counts())\n",
        "# there is a majority class of 0 here, hence the classes are skewed\n",
        "\n",
        "# calculate null accuracy (for multi-class classification problems)\n",
        "# .head(1) assesses the value 1208\n",
        "null_accuracy = y_test.value_counts().head(1) / len(y_test)\n",
        "print('Null accuracy:', null_accuracy)\n",
        "\n",
        "# Manual calculation of null accuracy by always predicting the majority class\n",
        "print('Manual null accuracy:',(1208 / (1208 + 185)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taliban                                                        2030\n",
            "Islamic State of Iraq and the Levant (ISIL)                    1470\n",
            "Al-Shabaab                                                      904\n",
            "Boko Haram                                                      681\n",
            "Communist Party of India - Maoist (CPI-Maoist)                  507\n",
            "New People's Army (NPA)                                         500\n",
            "Maoists                                                         431\n",
            "Revolutionary Armed Forces of Colombia (FARC)                   376\n",
            "Tehrik-i-Taliban Pakistan (TTP)                                 366\n",
            "Kurdistan Workers' Party (PKK)                                  346\n",
            "Houthi extremists (Ansar Allah)                                 293\n",
            "Al-Qaida in the Arabian Peninsula (AQAP)                        277\n",
            "Muslim extremists                                               172\n",
            "Liberation Tigers of Tamil Eelam (LTTE)                         170\n",
            "Donetsk People's Republic                                       169\n",
            "Al-Qaida in Iraq                                                165\n",
            "Abu Sayyaf Group (ASG)                                          144\n",
            "Fulani extremists                                               144\n",
            "Palestinian Extremists                                          113\n",
            "National Liberation Army of Colombia (ELN)                      111\n",
            "Sinai Province of the Islamic State                             106\n",
            "Separatists                                                     104\n",
            "Hamas (Islamic Resistance Movement)                              78\n",
            "Basque Fatherland and Freedom (ETA)                              69\n",
            "National Union for the Total Independence of Angola (UNITA)      40\n",
            "Corsican National Liberation Front (FLNC)                        37\n",
            "Hezbollah                                                        33\n",
            "Shining Path (SL)                                                23\n",
            "Irish Republican Army (IRA)                                       8\n",
            "Palestinians                                                      7\n",
            "Sikh Extremists                                                   1\n",
            "Name: gname, dtype: int64\n",
            "Null accuracy: Taliban    0.20557\n",
            "Name: gname, dtype: float64\n",
            "Manual null accuracy: 0.8671931083991385\n"
          ]
        }
      ],
      "metadata": {
        "id": "IooGVbA7gHEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7b99ec-4b40-4c0c-b201-9525095bab57"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "source": [
        "# print the confusion matrix\n",
        "# metrics.confusion_matrix(y_test, y_pred_class)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gicAbrV2u98q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "33f0cafb-792b-4b52-f93f-7a3f2d08d5f1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "# print message text for the false positives (ham incorrectly classified as spam)\n",
        "\n",
        "X_test[y_pred_class > y_test]\n",
        "\n",
        "# alternative less elegant but easier to understand\n",
        "# X_test[(y_pred_class==1) & (y_test==0)]"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_pred_class' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-b6e5c0a293b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print message text for the false positives (ham incorrectly classified as spam)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred_class\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# alternative less elegant but easier to understand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred_class' is not defined"
          ]
        }
      ],
      "metadata": {
        "id": "7imUVuACu91O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# print message text for the false negatives (spam incorrectly classified as ham)\n",
        "\n",
        "X_test[y_pred_class < y_test]\n",
        "# alternative less elegant but easier to understand\n",
        "# X_test[(y_pred_class=0) & (y_test=1)]\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "plcy2VRTu9uX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "# example false negative\n",
        "X_test[3132]"
      ],
      "outputs": [],
      "metadata": {
        "id": "HSMuGFqmu9lN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "source": [
        "# calculate predicted probabilities for X_test_dtm (poorly calibrated)\n",
        "\n",
        "# Numpy Array with 2C\n",
        "# left Column: probability class 0\n",
        "# right C: probability class 1\n",
        "# we only need the right column \n",
        "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
        "y_pred_prob\n",
        "\n",
        "# Naive Bayes predicts very extreme probabilites, you should not take them at face value"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_test_dtm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-5c1975110ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# right C: probability class 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# we only need the right column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_pred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_dtm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_pred_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test_dtm' is not defined"
          ]
        }
      ],
      "metadata": {
        "id": "HZnx8gswvGXN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# calculate AUC\n",
        "metrics.roc_auc_score(y_test, y_pred_prob)\n",
        "# AUC is useful as a single number summary of classifier performance\n",
        "# Higher value = better classifier\n",
        "# If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a higher predicted probability to the positive observation\n",
        "# AUC is useful even when there is high class imbalance (unlike classification accuracy)\n",
        "# Fraud case\n",
        "# Null accuracy almost 99%\n",
        "# AUC is useful here"
      ],
      "outputs": [],
      "metadata": {
        "id": "500zp9ENvMW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP"
      ],
      "metadata": {
        "id": "4KYzkjAjBX-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# temp_corpus = X['summary'].dropna()\n",
        "# corpus = temp_corpus.apply(lambda x: x.split(': ')[-1]).values\n",
        "# # print(f'We have {len(corpus)} elements on the corpus\\n\\n')\n",
        "# # print(f'Example 1: \\n{corpus[1]}\\n')\n",
        "# # print(f'Example 2: \\n{corpus[-1]}')\n",
        "\n",
        "# corpus"
      ],
      "outputs": [],
      "metadata": {
        "id": "AFfinVKRGu-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "D_GPeIPOGvBP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "kwjVzuthEZWY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "33TUx9dUGvG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from scipy.stats import zscore\n",
        "# from sklearn.cluster import DBSCAN\n",
        "# from sklearn.covariance import EllipticEnvelope\n",
        "# from sklearn.ensemble import IsolationForest\n",
        "# from sklearn.neighbors import LocalOutlierFactor\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.svm import OneClassSVM\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.metrics import accuracy_score,f1_score,confusion_matrix\n",
        "# import matplotlib.pyplot as plt\n",
        "# from matplotlib.pyplot import pie\n",
        "# %matplotlib inline\n",
        "# import time\n",
        "# import pandas_profiling\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.feature_selection import f_classif\n",
        "# from xgboost import XGBClassifier\n",
        "# from hyperopt import hp,Trials,fmin,tpe"
      ],
      "outputs": [],
      "metadata": {
        "id": "u3VEMZAvGvKQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "df2 = data[data['gname'].isin(counts[counts > 700].index)]"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "df2['category_id'] = df2['gname'].factorize()[0]\n",
        "category_id_df = df2[['gname', 'category_id']].drop_duplicates().sort_values('category_id')\n",
        "category_to_id = dict(category_id_df.values)\n",
        "id_to_category = dict(category_id_df[['category_id', 'gname']].values)\n",
        "df2.head()\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
        "features = tfidf.fit_transform(df.summary).toarray()\n",
        "labels = df.gname\n",
        "features.shape"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-e6d41a60e823>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df2['category_id'] = df2['gname'].factorize()[0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32916, 21593)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "import numpy as np\n",
        "N = 2\n",
        "for Product, category_id in sorted(category_to_id.items()):\n",
        "  features_chi2 = chi2(features, labels == category_id)\n",
        "  indices = np.argsort(features_chi2[0])\n",
        "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "  print(\"# '{}':\".format(Product))\n",
        "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
        "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# 'Al-Qaida in the Arabian Peninsula (AQAP)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Al-Shabaab':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Basque Fatherland and Freedom (ETA)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Boko Haram':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Communist Party of India - Maoist (CPI-Maoist)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Houthi extremists (Ansar Allah)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Irish Republican Army (IRA)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Islamic State of Iraq and the Levant (ISIL)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Kurdistan Workers' Party (PKK)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Liberation Tigers of Tamil Eelam (LTTE)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Maoists':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'National Liberation Army of Colombia (ELN)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'New People's Army (NPA)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Palestinians':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Revolutionary Armed Forces of Colombia (FARC)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Shining Path (SL)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Sikh Extremists':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Taliban':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n",
            "# 'Tehrik-i-Taliban Pakistan (TTP)':\n",
            "  . Most correlated unigrams:\n",
            ". escorts\n",
            ". escorting\n",
            "  . Most correlated bigrams:\n",
            ". essar steel\n",
            ". zuweid town\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}